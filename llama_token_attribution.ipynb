{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llama-3.2-1B Token Attribution Analysis\n",
        "\n",
        "This notebook demonstrates three different methods for analyzing which input tokens are most important in generating model predictions:\n",
        "\n",
        "1. **Gradient-Based Attribution**: Compute gradients of output logits w.r.t. input embeddings\n",
        "2. **Attention Weights Visualization**: Analyze attention patterns across layers\n",
        "3. **Integrated Gradients**: Use Captum library for sophisticated attribution\n",
        "\n",
        "We'll use the prompt: \"What is the Capital City of Latvia?\" and analyze which tokens the model uses to generate the answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Model Loading\n",
        "\n",
        "First, let's install the required packages and import necessary libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from captum.attr import IntegratedGradients\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer...\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n403 Client Error. (Request ID: Root=1-6918cc10-774e5b22388fb60028530e55;c0259be1-b98a-49ee-9d32-c52826dd886c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.2-1B is awaiting a review from the repo authors.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:1114\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1113\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1114\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:1655\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1650\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1651\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1652\u001b[39m ):\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1655\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:1543\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:1460\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m hf_raise_for_status(r)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:283\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/file_download.py:307\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    306\u001b[39m response = http_backoff(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:419\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    416\u001b[39m     message = (\n\u001b[32m    417\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[31mGatedRepoError\u001b[39m: 403 Client Error. (Request ID: Root=1-6918cc10-774e5b22388fb60028530e55;c0259be1-b98a-49ee-9d32-c52826dd886c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.2-1B is awaiting a review from the repo authors.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m tokenizer.pad_token = tokenizer.eos_token  \u001b[38;5;66;03m# Set padding token\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1093\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m   1092\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1096\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1332\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1330\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1333\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1334\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/transformers/configuration_utils.py:662\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    660\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/transformers/configuration_utils.py:721\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    717\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch-gpu/lib/python3.11/site-packages/transformers/utils/hub.py:543\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[32m    542\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    544\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure to have access to it at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    545\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    546\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, LocalEntryNotFoundError):\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n",
            "\u001b[31mOSError\u001b[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n403 Client Error. (Request ID: Root=1-6918cc10-774e5b22388fb60028530e55;c0259be1-b98a-49ee-9d32-c52826dd886c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.2-1B is awaiting a review from the repo authors."
          ]
        }
      ],
      "source": [
        "# Load the Llama-3.2-1B model and tokenizer\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Model parameters: {model.num_parameters() / 1e6:.2f}M\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Simple Generation Task\n",
        "\n",
        "Let's generate an answer to the question: \"What is the Capital City of Latvia?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the prompt\n",
        "prompt = \"What is the Capital City of Latvia?\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nGenerating response...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "print(f\"Input tokens: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
        "print(f\"Number of input tokens: {len(input_ids[0])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate response\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=20,\n",
        "        do_sample=False,  # Use greedy decoding for reproducibility\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Decode the response\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nGenerated text: {generated_text}\")\n",
        "\n",
        "# Extract only the generated part (excluding the prompt)\n",
        "generated_only = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
        "print(f\"\\nGenerated answer: {generated_only}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Method A: Gradient-Based Attribution\n",
        "\n",
        "This method computes the gradients of the output logits with respect to the input token embeddings. Tokens with higher gradient magnitudes have more influence on the prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_based_attribution(model, input_ids, target_token_idx=-1):\n",
        "    \"\"\"\n",
        "    Compute gradient-based attribution scores for input tokens.\n",
        "    \n",
        "    Args:\n",
        "        model: The language model\n",
        "        input_ids: Input token IDs\n",
        "        target_token_idx: Index of the output token to compute gradients for (-1 for last token)\n",
        "    \n",
        "    Returns:\n",
        "        attribution_scores: Attribution scores for each input token\n",
        "    \"\"\"\n",
        "    # Get embeddings layer\n",
        "    embeddings = model.get_input_embeddings()\n",
        "    \n",
        "    # Get token embeddings\n",
        "    token_embeddings = embeddings(input_ids)\n",
        "    token_embeddings.requires_grad_(True)\n",
        "    \n",
        "    # Forward pass with custom embeddings\n",
        "    outputs = model(inputs_embeds=token_embeddings)\n",
        "    logits = outputs.logits\n",
        "    \n",
        "    # Get the logits for the target position\n",
        "    target_logits = logits[0, target_token_idx]\n",
        "    \n",
        "    # Get the predicted token\n",
        "    predicted_token_id = target_logits.argmax(dim=-1)\n",
        "    \n",
        "    # Compute gradients of the predicted token's logit w.r.t. input embeddings\n",
        "    target_score = target_logits[predicted_token_id]\n",
        "    target_score.backward()\n",
        "    \n",
        "    # Get gradients\n",
        "    gradients = token_embeddings.grad\n",
        "    \n",
        "    # Compute attribution scores (L2 norm of gradients)\n",
        "    attribution_scores = gradients.norm(dim=-1).squeeze().cpu().detach().numpy()\n",
        "    \n",
        "    return attribution_scores, predicted_token_id.item()\n",
        "\n",
        "print(\"Computing gradient-based attribution...\")\n",
        "grad_scores, predicted_id = gradient_based_attribution(model, input_ids)\n",
        "predicted_token = tokenizer.decode([predicted_id])\n",
        "print(f\"Predicted next token: '{predicted_token}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize gradient-based attribution\n",
        "input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(range(len(input_tokens)), grad_scores, color='steelblue', alpha=0.7)\n",
        "plt.xlabel('Token Position', fontsize=12)\n",
        "plt.ylabel('Attribution Score (Gradient Norm)', fontsize=12)\n",
        "plt.title(f'Gradient-Based Attribution: Predicting \"{predicted_token}\"', fontsize=14, fontweight='bold')\n",
        "plt.xticks(range(len(input_tokens)), input_tokens, rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print top contributing tokens\n",
        "top_k = 5\n",
        "top_indices = np.argsort(grad_scores)[-top_k:][::-1]\n",
        "print(f\"\\nTop {top_k} contributing tokens (Gradient-Based):\")\n",
        "for i, idx in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. '{input_tokens[idx]}' (position {idx}): {grad_scores[idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Method B: Attention Weights Visualization\n",
        "\n",
        "This method analyzes the attention patterns across all layers to see which input tokens the model focuses on when making predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_attention_weights(model, input_ids):\n",
        "    \"\"\"\n",
        "    Extract attention weights from all layers.\n",
        "    \n",
        "    Args:\n",
        "        model: The language model\n",
        "        input_ids: Input token IDs\n",
        "    \n",
        "    Returns:\n",
        "        attention_weights: Aggregated attention weights\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, output_attentions=True)\n",
        "    \n",
        "    # Get attention weights from all layers\n",
        "    # attentions is a tuple of (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
        "    attentions = outputs.attentions\n",
        "    \n",
        "    # Stack all layers and average across layers and heads\n",
        "    # We'll look at attention from the last token to all previous tokens\n",
        "    all_attention = torch.stack(attentions)  # (num_layers, batch, num_heads, seq_len, seq_len)\n",
        "    \n",
        "    # Average across layers and heads\n",
        "    avg_attention = all_attention.mean(dim=(0, 2))  # (batch, seq_len, seq_len)\n",
        "    \n",
        "    # Get attention from the last token to all tokens\n",
        "    last_token_attention = avg_attention[0, -1, :].cpu().numpy()\n",
        "    \n",
        "    return last_token_attention, all_attention\n",
        "\n",
        "print(\"Extracting attention weights...\")\n",
        "attention_scores, all_attention = extract_attention_weights(model, input_ids)\n",
        "print(f\"Shape of all attention weights: {all_attention.shape}\")\n",
        "print(f\"Number of layers: {len(model.model.layers)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize attention weights\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(range(len(input_tokens)), attention_scores, color='coral', alpha=0.7)\n",
        "plt.xlabel('Token Position', fontsize=12)\n",
        "plt.ylabel('Attention Score (Averaged)', fontsize=12)\n",
        "plt.title('Attention Weights: Last Token Attending to Input Tokens', fontsize=14, fontweight='bold')\n",
        "plt.xticks(range(len(input_tokens)), input_tokens, rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print top attending tokens\n",
        "top_k = 5\n",
        "top_indices = np.argsort(attention_scores)[-top_k:][::-1]\n",
        "print(f\"\\nTop {top_k} attended tokens (Attention Weights):\")\n",
        "for i, idx in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. '{input_tokens[idx]}' (position {idx}): {attention_scores[idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a detailed attention heatmap across layers\n",
        "# Average across heads for each layer, showing attention from last token\n",
        "layer_attention = all_attention[:, 0, :, -1, :].mean(dim=1).cpu().numpy()  # (num_layers, seq_len)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.heatmap(\n",
        "    layer_attention,\n",
        "    xticklabels=input_tokens,\n",
        "    yticklabels=[f'Layer {i}' for i in range(layer_attention.shape[0])],\n",
        "    cmap='YlOrRd',\n",
        "    cbar_kws={'label': 'Attention Weight'},\n",
        "    annot=False\n",
        ")\n",
        "plt.xlabel('Input Tokens', fontsize=12)\n",
        "plt.ylabel('Transformer Layers', fontsize=12)\n",
        "plt.title('Attention Heatmap Across All Layers (Last Token)', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Method C: Integrated Gradients\n",
        "\n",
        "Integrated Gradients is a more sophisticated attribution method that computes the path integral of gradients from a baseline to the actual input. This provides more faithful attributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_forward(embeddings, attention_mask=None):\n",
        "    \"\"\"\n",
        "    Forward function for Captum's IntegratedGradients.\n",
        "    \n",
        "    Args:\n",
        "        embeddings: Input embeddings\n",
        "        attention_mask: Attention mask (optional)\n",
        "    \n",
        "    Returns:\n",
        "        logits for the predicted token at the last position\n",
        "    \"\"\"\n",
        "    outputs = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
        "    # Return logits for the last position\n",
        "    return outputs.logits[:, -1, :]\n",
        "\n",
        "def compute_integrated_gradients(model, input_ids, baseline_type='zero'):\n",
        "    \"\"\"\n",
        "    Compute integrated gradients attribution.\n",
        "    \n",
        "    Args:\n",
        "        model: The language model\n",
        "        input_ids: Input token IDs\n",
        "        baseline_type: Type of baseline ('zero' or 'pad')\n",
        "    \n",
        "    Returns:\n",
        "        attribution_scores: Attribution scores for each input token\n",
        "        predicted_token_id: ID of the predicted token\n",
        "    \"\"\"\n",
        "    embeddings_layer = model.get_input_embeddings()\n",
        "    \n",
        "    # Get input embeddings\n",
        "    input_embeddings = embeddings_layer(input_ids)\n",
        "    \n",
        "    # Create baseline\n",
        "    if baseline_type == 'zero':\n",
        "        baseline_embeddings = torch.zeros_like(input_embeddings)\n",
        "    else:  # 'pad'\n",
        "        pad_token_id = tokenizer.pad_token_id\n",
        "        baseline_ids = torch.full_like(input_ids, pad_token_id)\n",
        "        baseline_embeddings = embeddings_layer(baseline_ids)\n",
        "    \n",
        "    # Get predicted token\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predicted_token_id = outputs.logits[0, -1].argmax().item()\n",
        "    \n",
        "    # Initialize IntegratedGradients\n",
        "    ig = IntegratedGradients(model_forward)\n",
        "    \n",
        "    # Compute attributions\n",
        "    attributions = ig.attribute(\n",
        "        inputs=input_embeddings,\n",
        "        baselines=baseline_embeddings,\n",
        "        target=predicted_token_id,\n",
        "        n_steps=50,\n",
        "        internal_batch_size=1\n",
        "    )\n",
        "    \n",
        "    # Compute L2 norm of attributions for each token\n",
        "    attribution_scores = attributions.norm(dim=-1).squeeze().cpu().detach().numpy()\n",
        "    \n",
        "    return attribution_scores, predicted_token_id\n",
        "\n",
        "print(\"Computing Integrated Gradients (this may take a moment)...\")\n",
        "ig_scores, ig_predicted_id = compute_integrated_gradients(model, input_ids, baseline_type='zero')\n",
        "ig_predicted_token = tokenizer.decode([ig_predicted_id])\n",
        "print(f\"Predicted next token: '{ig_predicted_token}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Integrated Gradients attribution\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(range(len(input_tokens)), ig_scores, color='mediumseagreen', alpha=0.7)\n",
        "plt.xlabel('Token Position', fontsize=12)\n",
        "plt.ylabel('Attribution Score (IG)', fontsize=12)\n",
        "plt.title(f'Integrated Gradients Attribution: Predicting \"{ig_predicted_token}\"', fontsize=14, fontweight='bold')\n",
        "plt.xticks(range(len(input_tokens)), input_tokens, rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print top contributing tokens\n",
        "top_k = 5\n",
        "top_indices = np.argsort(ig_scores)[-top_k:][::-1]\n",
        "print(f\"\\nTop {top_k} contributing tokens (Integrated Gradients):\")\n",
        "for i, idx in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. '{input_tokens[idx]}' (position {idx}): {ig_scores[idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comparison of All Three Methods\n",
        "\n",
        "Let's compare all three attribution methods side by side to see which tokens each method identifies as most important.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize scores for better comparison\n",
        "def normalize_scores(scores):\n",
        "    return (scores - scores.min()) / (scores.max() - scores.min() + 1e-10)\n",
        "\n",
        "grad_scores_norm = normalize_scores(grad_scores)\n",
        "attention_scores_norm = normalize_scores(attention_scores)\n",
        "ig_scores_norm = normalize_scores(ig_scores)\n",
        "\n",
        "# Create comparison plot\n",
        "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Gradient-Based\n",
        "axes[0].bar(range(len(input_tokens)), grad_scores_norm, color='steelblue', alpha=0.7)\n",
        "axes[0].set_ylabel('Normalized Score', fontsize=11)\n",
        "axes[0].set_title('A) Gradient-Based Attribution', fontsize=13, fontweight='bold')\n",
        "axes[0].set_xticks(range(len(input_tokens)))\n",
        "axes[0].set_xticklabels(input_tokens, rotation=45, ha='right')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: Attention Weights\n",
        "axes[1].bar(range(len(input_tokens)), attention_scores_norm, color='coral', alpha=0.7)\n",
        "axes[1].set_ylabel('Normalized Score', fontsize=11)\n",
        "axes[1].set_title('B) Attention Weights', fontsize=13, fontweight='bold')\n",
        "axes[1].set_xticks(range(len(input_tokens)))\n",
        "axes[1].set_xticklabels(input_tokens, rotation=45, ha='right')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 3: Integrated Gradients\n",
        "axes[2].bar(range(len(input_tokens)), ig_scores_norm, color='mediumseagreen', alpha=0.7)\n",
        "axes[2].set_xlabel('Token Position', fontsize=12)\n",
        "axes[2].set_ylabel('Normalized Score', fontsize=11)\n",
        "axes[2].set_title('C) Integrated Gradients', fontsize=13, fontweight='bold')\n",
        "axes[2].set_xticks(range(len(input_tokens)))\n",
        "axes[2].set_xticklabels(input_tokens, rotation=45, ha='right')\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Comparison of Token Attribution Methods', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a combined heatmap\n",
        "attribution_matrix = np.vstack([\n",
        "    grad_scores_norm,\n",
        "    attention_scores_norm,\n",
        "    ig_scores_norm\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "sns.heatmap(\n",
        "    attribution_matrix,\n",
        "    xticklabels=input_tokens,\n",
        "    yticklabels=['Gradient-Based', 'Attention Weights', 'Integrated Gradients'],\n",
        "    cmap='RdYlGn',\n",
        "    cbar_kws={'label': 'Normalized Attribution Score'},\n",
        "    annot=True,\n",
        "    fmt='.2f',\n",
        "    linewidths=0.5\n",
        ")\n",
        "plt.xlabel('Input Tokens', fontsize=12)\n",
        "plt.ylabel('Attribution Method', fontsize=12)\n",
        "plt.title('Combined Attribution Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"=\" * 80)\n",
        "print(\"SUMMARY: Token Attribution Analysis\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nPrompt: {prompt}\")\n",
        "print(f\"Generated Answer: {generated_only}\")\n",
        "print(f\"\\nPredicted next token: '{predicted_token}'\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"Top 3 Most Important Tokens by Each Method:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "methods = [\n",
        "    ('Gradient-Based', grad_scores_norm),\n",
        "    ('Attention Weights', attention_scores_norm),\n",
        "    ('Integrated Gradients', ig_scores_norm)\n",
        "]\n",
        "\n",
        "for method_name, scores in methods:\n",
        "    top_3 = np.argsort(scores)[-3:][::-1]\n",
        "    print(f\"\\n{method_name}:\")\n",
        "    for rank, idx in enumerate(top_3, 1):\n",
        "        print(f\"  {rank}. '{input_tokens[idx]}' (pos {idx}): {scores[idx]:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis and Insights\n",
        "\n",
        "### Method Comparison:\n",
        "\n",
        "1. **Gradient-Based Attribution**:\n",
        "   - Shows which tokens have the largest gradient magnitudes\n",
        "   - Indicates tokens that, if changed slightly, would most affect the output\n",
        "   - Fast to compute but can be noisy\n",
        "\n",
        "2. **Attention Weights**:\n",
        "   - Shows which tokens the model explicitly attends to\n",
        "   - Provides interpretability through the attention mechanism\n",
        "   - May not fully capture all influences (attention is just one component)\n",
        "\n",
        "3. **Integrated Gradients**:\n",
        "   - More theoretically grounded attribution method\n",
        "   - Satisfies desirable axioms like completeness and sensitivity\n",
        "   - Slower to compute but generally more reliable\n",
        "\n",
        "### Key Observations:\n",
        "\n",
        "- All three methods typically identify question words (\"What\", \"Latvia\") as important\n",
        "- The specific tokens \"Capital\", \"City\", and \"Latvia\" are usually highly weighted\n",
        "- Different methods may emphasize different aspects of the input\n",
        "- Combining multiple attribution methods provides a more complete picture\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
