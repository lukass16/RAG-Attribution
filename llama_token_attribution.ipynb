{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llama-3.2-1B Token Attribution Analysis\n",
        "\n",
        "This notebook demonstrates three different methods for analyzing which input tokens are most important in generating model predictions:\n",
        "\n",
        "1. **Gradient-Based Attribution**: Compute gradients of output logits w.r.t. input embeddings\n",
        "2. **Attention Weights Visualization**: Analyze attention patterns across layers\n",
        "3. **Integrated Gradients**: Use Captum library for sophisticated attribution\n",
        "\n",
        "We'll use the prompt: \"What is the Capital City of Latvia?\" and analyze which tokens the model uses to generate the answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Model Loading\n",
        "\n",
        "First, let's install the required packages and import necessary libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from captum.attr import IntegratedGradients\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Llama-3.2-1B model and tokenizer\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Model parameters: {model.num_parameters() / 1e6:.2f}M\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Simple Generation Task\n",
        "\n",
        "Let's generate an answer to the question: \"What is the Capital City of Latvia?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the prompt\n",
        "prompt = \"What is the Capital City of Latvia?\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nGenerating response...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "print(f\"Input tokens: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
        "print(f\"Number of input tokens: {len(input_ids[0])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate response\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=20,\n",
        "        do_sample=False,  # Use greedy decoding for reproducibility\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Decode the response\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nGenerated text: {generated_text}\")\n",
        "\n",
        "# Extract only the generated part (excluding the prompt)\n",
        "generated_only = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
        "print(f\"\\nGenerated answer: {generated_only}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Method A: Gradient-Based Attribution\n",
        "\n",
        "This method computes the gradients of the output logits with respect to the input token embeddings. Tokens with higher gradient magnitudes have more influence on the prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_based_attribution(model, input_ids, target_token_idx=-1):\n",
        "    \"\"\"\n",
        "    Compute gradient-based attribution scores for input tokens.\n",
        "    \n",
        "    Args:\n",
        "        model: The language model\n",
        "        input_ids: Input token IDs\n",
        "        target_token_idx: Index of the output token to compute gradients for (-1 for last token)\n",
        "    \n",
        "    Returns:\n",
        "        attribution_scores: Attribution scores for each input token\n",
        "    \"\"\"\n",
        "    # Ensure gradients are enabled\n",
        "    with torch.enable_grad():\n",
        "        # Get embeddings layer and its weight matrix\n",
        "        embeddings_layer = model.get_input_embeddings()\n",
        "        embedding_matrix = embeddings_layer.weight  # Shape: (vocab_size, embedding_dim)\n",
        "        \n",
        "        # Manually create embeddings by indexing the weight matrix\n",
        "        # This ensures we build a proper computation graph\n",
        "        token_embeddings = embedding_matrix[input_ids]  # Shape: (batch, seq_len, embedding_dim)\n",
        "        \n",
        "        # Detach and clone to break any existing graph, then enable gradients\n",
        "        token_embeddings = token_embeddings.detach().clone()\n",
        "        token_embeddings.requires_grad_(True)\n",
        "        \n",
        "        # Forward pass with custom embeddings\n",
        "        outputs = model(inputs_embeds=token_embeddings)\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        # Get the logits for the target position\n",
        "        target_logits = logits[0, target_token_idx]\n",
        "        \n",
        "        # Get the predicted token\n",
        "        predicted_token_id = target_logits.argmax(dim=-1)\n",
        "        \n",
        "        # Compute gradients of the predicted token's logit w.r.t. input embeddings\n",
        "        target_score = target_logits[predicted_token_id]\n",
        "        target_score.backward()\n",
        "        \n",
        "        # Get gradients\n",
        "        gradients = token_embeddings.grad\n",
        "        \n",
        "        # Check if gradients were computed\n",
        "        if gradients is None:\n",
        "            raise RuntimeError(\"Gradients were not computed. This may indicate an issue with the computation graph.\")\n",
        "        \n",
        "        # Compute attribution scores (L2 norm of gradients)\n",
        "        attribution_scores = gradients.norm(dim=-1).squeeze().cpu().detach().numpy()\n",
        "    \n",
        "    return attribution_scores, predicted_token_id.item()\n",
        "\n",
        "print(\"Computing gradient-based attribution...\")\n",
        "grad_scores, predicted_id = gradient_based_attribution(model, input_ids)\n",
        "predicted_token = tokenizer.decode([predicted_id])\n",
        "print(f\"Predicted next token: '{predicted_token}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize gradient-based attribution\n",
        "input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(range(len(input_tokens)), grad_scores, color='steelblue', alpha=0.7)\n",
        "plt.xlabel('Token Position', fontsize=12)\n",
        "plt.ylabel('Attribution Score (Gradient Norm)', fontsize=12)\n",
        "plt.title(f'Gradient-Based Attribution: Predicting \"{predicted_token}\"', fontsize=14, fontweight='bold')\n",
        "plt.xticks(range(len(input_tokens)), input_tokens, rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print top contributing tokens\n",
        "top_k = 5\n",
        "top_indices = np.argsort(grad_scores)[-top_k:][::-1]\n",
        "print(f\"\\nTop {top_k} contributing tokens (Gradient-Based):\")\n",
        "for i, idx in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. '{input_tokens[idx]}' (position {idx}): {grad_scores[idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Method B: Attention Weights Visualization\n",
        "\n",
        "This method analyzes the attention patterns across all layers to see which input tokens the model focuses on when making predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_attention_weights(model, input_ids):\n",
        "    \"\"\"\n",
        "    Extract attention weights from all layers.\n",
        "    \n",
        "    Args:\n",
        "        model: The language model\n",
        "        input_ids: Input token IDs\n",
        "    \n",
        "    Returns:\n",
        "        attention_weights: Aggregated attention weights\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, output_attentions=True)\n",
        "    \n",
        "    # Get attention weights from all layers\n",
        "    # attentions is a tuple of (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
        "    attentions = outputs.attentions\n",
        "    \n",
        "    # Stack all layers and average across layers and heads\n",
        "    # We'll look at attention from the last token to all previous tokens\n",
        "    all_attention = torch.stack(attentions)  # (num_layers, batch, num_heads, seq_len, seq_len)\n",
        "    \n",
        "    # Average across layers and heads\n",
        "    avg_attention = all_attention.mean(dim=(0, 2))  # (batch, seq_len, seq_len)\n",
        "    \n",
        "    # Get attention from the last token to all tokens\n",
        "    last_token_attention = avg_attention[0, -1, :].cpu().numpy()\n",
        "    \n",
        "    return last_token_attention, all_attention\n",
        "\n",
        "print(\"Extracting attention weights...\")\n",
        "attention_scores, all_attention = extract_attention_weights(model, input_ids)\n",
        "print(f\"Shape of all attention weights: {all_attention.shape}\")\n",
        "print(f\"Number of layers: {len(model.model.layers)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize attention weights\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(range(len(input_tokens)), attention_scores, color='coral', alpha=0.7)\n",
        "plt.xlabel('Token Position', fontsize=12)\n",
        "plt.ylabel('Attention Score (Averaged)', fontsize=12)\n",
        "plt.title('Attention Weights: Last Token Attending to Input Tokens', fontsize=14, fontweight='bold')\n",
        "plt.xticks(range(len(input_tokens)), input_tokens, rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print top attending tokens\n",
        "top_k = 5\n",
        "top_indices = np.argsort(attention_scores)[-top_k:][::-1]\n",
        "print(f\"\\nTop {top_k} attended tokens (Attention Weights):\")\n",
        "for i, idx in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. '{input_tokens[idx]}' (position {idx}): {attention_scores[idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a detailed attention heatmap across layers\n",
        "# Average across heads for each layer, showing attention from last token\n",
        "layer_attention = all_attention[:, 0, :, -1, :].mean(dim=1).cpu().numpy()  # (num_layers, seq_len)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.heatmap(\n",
        "    layer_attention,\n",
        "    xticklabels=input_tokens,\n",
        "    yticklabels=[f'Layer {i}' for i in range(layer_attention.shape[0])],\n",
        "    cmap='YlOrRd',\n",
        "    cbar_kws={'label': 'Attention Weight'},\n",
        "    annot=False\n",
        ")\n",
        "plt.xlabel('Input Tokens', fontsize=12)\n",
        "plt.ylabel('Transformer Layers', fontsize=12)\n",
        "plt.title('Attention Heatmap Across All Layers (Last Token)', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Method C: Integrated Gradients\n",
        "\n",
        "Integrated Gradients is a more sophisticated attribution method that computes the path integral of gradients from a baseline to the actual input. This provides more faithful attributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_forward(embeddings, attention_mask=None):\n",
        "    \"\"\"\n",
        "    Forward function for Captum's IntegratedGradients.\n",
        "    \n",
        "    Args:\n",
        "        embeddings: Input embeddings\n",
        "        attention_mask: Attention mask (optional)\n",
        "    \n",
        "    Returns:\n",
        "        logits for the predicted token at the last position\n",
        "    \"\"\"\n",
        "    outputs = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
        "    # Return logits for the last position\n",
        "    return outputs.logits[:, -1, :]\n",
        "\n",
        "def compute_integrated_gradients(model, input_ids, baseline_type='zero'):\n",
        "    \"\"\"\n",
        "    Compute integrated gradients attribution.\n",
        "    \n",
        "    Args:\n",
        "        model: The language model\n",
        "        input_ids: Input token IDs\n",
        "        baseline_type: Type of baseline ('zero' or 'pad')\n",
        "    \n",
        "    Returns:\n",
        "        attribution_scores: Attribution scores for each input token\n",
        "        predicted_token_id: ID of the predicted token\n",
        "    \"\"\"\n",
        "    embeddings_layer = model.get_input_embeddings()\n",
        "    embedding_matrix = embeddings_layer.weight\n",
        "    \n",
        "    # Get input embeddings using manual indexing for proper gradient computation\n",
        "    # This ensures the computation graph is built correctly\n",
        "    input_embeddings = embedding_matrix[input_ids].detach().clone()\n",
        "    \n",
        "    # Create baseline\n",
        "    if baseline_type == 'zero':\n",
        "        baseline_embeddings = torch.zeros_like(input_embeddings)\n",
        "    else:  # 'pad'\n",
        "        pad_token_id = tokenizer.pad_token_id\n",
        "        baseline_ids = torch.full_like(input_ids, pad_token_id)\n",
        "        # Use manual indexing for baseline too\n",
        "        baseline_embeddings = embedding_matrix[baseline_ids].detach().clone()\n",
        "    \n",
        "    # Get predicted token\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predicted_token_id = outputs.logits[0, -1].argmax().item()\n",
        "    \n",
        "    # Initialize IntegratedGradients\n",
        "    # Captum will handle enabling gradients internally\n",
        "    ig = IntegratedGradients(model_forward)\n",
        "    \n",
        "    # Compute attributions\n",
        "    attributions = ig.attribute(\n",
        "        inputs=input_embeddings,\n",
        "        baselines=baseline_embeddings,\n",
        "        target=predicted_token_id,\n",
        "        n_steps=50,\n",
        "        internal_batch_size=1\n",
        "    )\n",
        "    \n",
        "    # Compute L2 norm of attributions for each token\n",
        "    attribution_scores = attributions.norm(dim=-1).squeeze().cpu().detach().numpy()\n",
        "    \n",
        "    return attribution_scores, predicted_token_id\n",
        "\n",
        "print(\"Computing Integrated Gradients (this may take a moment)...\")\n",
        "ig_scores, ig_predicted_id = compute_integrated_gradients(model, input_ids, baseline_type='zero')\n",
        "ig_predicted_token = tokenizer.decode([ig_predicted_id])\n",
        "print(f\"Predicted next token: '{ig_predicted_token}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Integrated Gradients attribution\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(range(len(input_tokens)), ig_scores, color='mediumseagreen', alpha=0.7)\n",
        "plt.xlabel('Token Position', fontsize=12)\n",
        "plt.ylabel('Attribution Score (IG)', fontsize=12)\n",
        "plt.title(f'Integrated Gradients Attribution: Predicting \"{ig_predicted_token}\"', fontsize=14, fontweight='bold')\n",
        "plt.xticks(range(len(input_tokens)), input_tokens, rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print top contributing tokens\n",
        "top_k = 5\n",
        "top_indices = np.argsort(ig_scores)[-top_k:][::-1]\n",
        "print(f\"\\nTop {top_k} contributing tokens (Integrated Gradients):\")\n",
        "for i, idx in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. '{input_tokens[idx]}' (position {idx}): {ig_scores[idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comparison of All Three Methods\n",
        "\n",
        "Let's compare all three attribution methods side by side to see which tokens each method identifies as most important.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize scores for better comparison\n",
        "def normalize_scores(scores):\n",
        "    return (scores - scores.min()) / (scores.max() - scores.min() + 1e-10)\n",
        "\n",
        "grad_scores_norm = normalize_scores(grad_scores)\n",
        "attention_scores_norm = normalize_scores(attention_scores)\n",
        "ig_scores_norm = normalize_scores(ig_scores)\n",
        "\n",
        "# Create comparison plot\n",
        "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Gradient-Based\n",
        "axes[0].bar(range(len(input_tokens)), grad_scores_norm, color='steelblue', alpha=0.7)\n",
        "axes[0].set_ylabel('Normalized Score', fontsize=11)\n",
        "axes[0].set_title('A) Gradient-Based Attribution', fontsize=13, fontweight='bold')\n",
        "axes[0].set_xticks(range(len(input_tokens)))\n",
        "axes[0].set_xticklabels(input_tokens, rotation=45, ha='right')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: Attention Weights\n",
        "axes[1].bar(range(len(input_tokens)), attention_scores_norm, color='coral', alpha=0.7)\n",
        "axes[1].set_ylabel('Normalized Score', fontsize=11)\n",
        "axes[1].set_title('B) Attention Weights', fontsize=13, fontweight='bold')\n",
        "axes[1].set_xticks(range(len(input_tokens)))\n",
        "axes[1].set_xticklabels(input_tokens, rotation=45, ha='right')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 3: Integrated Gradients\n",
        "axes[2].bar(range(len(input_tokens)), ig_scores_norm, color='mediumseagreen', alpha=0.7)\n",
        "axes[2].set_xlabel('Token Position', fontsize=12)\n",
        "axes[2].set_ylabel('Normalized Score', fontsize=11)\n",
        "axes[2].set_title('C) Integrated Gradients', fontsize=13, fontweight='bold')\n",
        "axes[2].set_xticks(range(len(input_tokens)))\n",
        "axes[2].set_xticklabels(input_tokens, rotation=45, ha='right')\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Comparison of Token Attribution Methods', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a combined heatmap\n",
        "attribution_matrix = np.vstack([\n",
        "    grad_scores_norm,\n",
        "    attention_scores_norm,\n",
        "    ig_scores_norm\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "sns.heatmap(\n",
        "    attribution_matrix,\n",
        "    xticklabels=input_tokens,\n",
        "    yticklabels=['Gradient-Based', 'Attention Weights', 'Integrated Gradients'],\n",
        "    cmap='RdYlGn',\n",
        "    cbar_kws={'label': 'Normalized Attribution Score'},\n",
        "    annot=True,\n",
        "    fmt='.2f',\n",
        "    linewidths=0.5\n",
        ")\n",
        "plt.xlabel('Input Tokens', fontsize=12)\n",
        "plt.ylabel('Attribution Method', fontsize=12)\n",
        "plt.title('Combined Attribution Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"=\" * 80)\n",
        "print(\"SUMMARY: Token Attribution Analysis\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nPrompt: {prompt}\")\n",
        "print(f\"Generated Answer: {generated_only}\")\n",
        "print(f\"\\nPredicted next token: '{predicted_token}'\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"Top 3 Most Important Tokens by Each Method:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "methods = [\n",
        "    ('Gradient-Based', grad_scores_norm),\n",
        "    ('Attention Weights', attention_scores_norm),\n",
        "    ('Integrated Gradients', ig_scores_norm)\n",
        "]\n",
        "\n",
        "for method_name, scores in methods:\n",
        "    top_3 = np.argsort(scores)[-3:][::-1]\n",
        "    print(f\"\\n{method_name}:\")\n",
        "    for rank, idx in enumerate(top_3, 1):\n",
        "        print(f\"  {rank}. '{input_tokens[idx]}' (pos {idx}): {scores[idx]:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis and Insights\n",
        "\n",
        "### Method Comparison:\n",
        "\n",
        "1. **Gradient-Based Attribution**:\n",
        "   - Shows which tokens have the largest gradient magnitudes\n",
        "   - Indicates tokens that, if changed slightly, would most affect the output\n",
        "   - Fast to compute but can be noisy\n",
        "\n",
        "2. **Attention Weights**:\n",
        "   - Shows which tokens the model explicitly attends to\n",
        "   - Provides interpretability through the attention mechanism\n",
        "   - May not fully capture all influences (attention is just one component)\n",
        "\n",
        "3. **Integrated Gradients**:\n",
        "   - More theoretically grounded attribution method\n",
        "   - Satisfies desirable axioms like completeness and sensitivity\n",
        "   - Slower to compute but generally more reliable\n",
        "\n",
        "### Key Observations:\n",
        "\n",
        "- All three methods typically identify question words (\"What\", \"Latvia\") as important\n",
        "- The specific tokens \"Capital\", \"City\", and \"Latvia\" are usually highly weighted\n",
        "- Different methods may emphasize different aspects of the input\n",
        "- Combining multiple attribution methods provides a more complete picture\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
