{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Source Attribution Analysis\n",
        "\n",
        "This notebook implements document-level source attribution for Retrieval-Augmented Generation (RAG) systems, based on the methodology from \"Source Attribution in Retrieval-Augmented Generation\" (arXiv:2507.04480).\n",
        "\n",
        "## Overview\n",
        "\n",
        "We will:\n",
        "1. Load query-document datasets from CSV/JSON files\n",
        "2. Generate target responses Rtarget using all documents\n",
        "3. Compute utility functions v(S) for document subsets\n",
        "4. Apply various attribution methods (Shapley values, approximations, baselines)\n",
        "5. Visualize and compare attribution results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "# Import our modules\n",
        "from rag_system import RAGSystem, load_dataset\n",
        "import attribution_methods as attr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Load Model and Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA H100 80GB HBM3\n",
            "Available memory: 85.02 GB\n"
          ]
        }
      ],
      "source": [
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing RAG system...\n",
            "Loading tokenizer for meta-llama/Llama-3.2-1B...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n",
            "Model loaded successfully on cuda\n",
            "RAG system ready!\n"
          ]
        }
      ],
      "source": [
        "# Initialize RAG system\n",
        "print(\"Initializing RAG system...\")\n",
        "rag = RAGSystem(model_name=\"meta-llama/Llama-3.2-1B\", device=str(device))\n",
        "print(\"RAG system ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from data/complementary.csv...\n",
            "Loaded 10 query-document pairs\n",
            "\n",
            "Sample query: What are the two primary materials used to construct a Xylotian 'Sky-Skiff' hull?\n",
            "Number of documents: 10\n",
            "Document IDs: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
            "\n",
            "First document: The lightweight frame of a Xylotian Sky-Skiff is primarily made from hardened 'Aero-Coral'....\n",
            "Answer: The two primary materials used to construct a Xylotian 'Sky-Skiff' hull are 'Aero-Coral' for the frame and 'Noctilucent Metal' for the cladding.\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "dataset_path = \"data/complementary.csv\"  # Change this to load different datasets\n",
        "print(f\"Loading dataset from {dataset_path}...\")\n",
        "dataset = load_dataset(dataset_path)\n",
        "print(f\"Loaded {len(dataset)} query-document pairs\")\n",
        "\n",
        "# Display sample\n",
        "if len(dataset) > 0:\n",
        "    sample = dataset[0]\n",
        "    print(f\"\\nSample query: {sample['question']}\")\n",
        "    print(f\"Number of documents: {len(sample['documents'])}\")\n",
        "    print(f\"Document IDs: {sample['document_ids']}\")\n",
        "    print(f\"\\nFirst document: {sample['documents'][0][:100]}...\")\n",
        "    print(f\"Answer: {sample['answer']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate Target Responses\n",
        "\n",
        "For each query-document pair (Q, D), generate Rtarget = LLM(Q, D) using ALL documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate target responses for all queries\n",
        "target_responses = []\n",
        "\n",
        "print(\"Generating target responses...\")\n",
        "for i, item in enumerate(dataset):\n",
        "    print(f\"Processing query {i+1}/{len(dataset)}...\")\n",
        "    rtarget = rag.generate_target_response(\n",
        "        question=item['question'],\n",
        "        all_documents=item['documents'],\n",
        "        max_new_tokens=50\n",
        "    )\n",
        "    target_responses.append(rtarget)\n",
        "    print(f\"  Generated: {rtarget[:80]}...\")\n",
        "\n",
        "print(f\"\\nGenerated {len(target_responses)} target responses\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compute Attribution Scores\n",
        "\n",
        "We'll compute attribution scores using multiple methods for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing query 1:\n",
            "Question: What are the two primary materials used to construct a Xylotian 'Sky-Skiff' hull?\n",
            "Number of documents: 10\n",
            "Target response: The primary materials used to construct a Xylotian 'Sky-Skiff' hull are 'Aero-Coral' and 'Noctilucent Metal'.\n",
            "Expected answer: The two primary materials used to construct a Xylotian 'Sky-Skiff' hull are 'Aero-Coral' for the frame and 'Noctilucent Metal' for the cladding.\n"
          ]
        }
      ],
      "source": [
        "# Select a query to analyze (you can change this index)\n",
        "query_idx = 0\n",
        "item = dataset[query_idx]\n",
        "target_response = target_responses[query_idx]\n",
        "\n",
        "print(f\"Analyzing query {query_idx + 1}:\")\n",
        "print(f\"Question: {item['question']}\")\n",
        "print(f\"Number of documents: {len(item['documents'])}\")\n",
        "print(f\"Target response: {target_response}\")\n",
        "print(f\"Expected answer: {item['answer']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create utility function for this query\n",
        "def utility_function(document_subset: List[str]) -> float:\n",
        "    \"\"\"Compute utility v(S) for document subset S.\"\"\"\n",
        "    return rag.compute_utility(\n",
        "        question=item['question'],\n",
        "        document_subset=document_subset,\n",
        "        target_response=target_response\n",
        "    )\n",
        "\n",
        "# Test utility function\n",
        "print(\"Testing utility function...\")\n",
        "print(f\"Utility with all documents: {utility_function(item['documents']):.4f}\")\n",
        "print(f\"Utility with first document only: {utility_function([item['documents'][0]]):.4f}\")\n",
        "print(f\"Utility with empty set: {utility_function([]):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Leave-One-Out attribution\n",
        "print(\"Computing Leave-One-Out attribution...\")\n",
        "loo_attributions = attr.leave_one_out(item['documents'], utility_function)\n",
        "print(\"Leave-One-Out scores:\")\n",
        "for i, doc_id in enumerate(item['document_ids']):\n",
        "    print(f\"  Document {doc_id}: {loo_attributions[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Monte Carlo Shapley (if we have reasonable number of documents)\n",
        "n_docs = len(item['documents'])\n",
        "print(f\"Computing Monte Carlo Shapley with {n_docs} documents...\")\n",
        "\n",
        "if n_docs <= 10:\n",
        "    mc_shapley = attr.monte_carlo_shapley(item['documents'], utility_function, num_samples=64)\n",
        "    print(\"Monte Carlo Shapley scores:\")\n",
        "    for i, doc_id in enumerate(item['document_ids']):\n",
        "        print(f\"  Document {doc_id}: {mc_shapley[i]:.4f}\")\n",
        "else:\n",
        "    print(f\"Skipping Monte Carlo Shapley - too many documents ({n_docs})\")\n",
        "    mc_shapley = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Permutation-based Shapley\n",
        "print(\"Computing Permutation-based Shapley...\")\n",
        "perm_shapley = attr.permutation_shapley(item['documents'], utility_function, num_permutations=50)\n",
        "print(\"Permutation Shapley scores:\")\n",
        "for i, doc_id in enumerate(item['document_ids']):\n",
        "    print(f\"  Document {doc_id}: {perm_shapley[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Kernel SHAP (if available and documents are reasonable)\n",
        "if n_docs <= 10:\n",
        "    print(\"Computing Kernel SHAP...\")\n",
        "    try:\n",
        "        kernel_shap = attr.kernel_shap(item['documents'], utility_function, num_samples=64)\n",
        "        print(\"Kernel SHAP scores:\")\n",
        "        for i, doc_id in enumerate(item['document_ids']):\n",
        "            print(f\"  Document {doc_id}: {kernel_shap[i]:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Kernel SHAP failed: {e}\")\n",
        "        kernel_shap = None\n",
        "else:\n",
        "    print(f\"Skipping Kernel SHAP - too many documents ({n_docs})\")\n",
        "    kernel_shap = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping Exact Shapley - requires 2^10 = 1024 evaluations\n"
          ]
        }
      ],
      "source": [
        "# Compute Exact Shapley (only for small document sets)\n",
        "if n_docs <= 5:\n",
        "    print(\"Computing Exact Shapley values...\")\n",
        "    try:\n",
        "        exact_shap = attr.exact_shapley(item['documents'], utility_function)\n",
        "        print(\"Exact Shapley scores:\")\n",
        "        for i, doc_id in enumerate(item['document_ids']):\n",
        "            print(f\"  Document {doc_id}: {exact_shap[i]:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Exact Shapley failed: {e}\")\n",
        "        exact_shap = None\n",
        "else:\n",
        "    print(f\"Skipping Exact Shapley - requires 2^{n_docs} = {2**n_docs} evaluations\")\n",
        "    exact_shap = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`sdpa` attention does not support `output_attentions=True` or `head_mask`. Please set your attention to `eager` if you want any of these features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing Attention-based attribution...\n",
            "Attention attribution failed: object of type 'NoneType' has no len()\n"
          ]
        }
      ],
      "source": [
        "# Compute Attention-based attribution\n",
        "print(\"Computing Attention-based attribution...\")\n",
        "try:\n",
        "    attention_attr = attr.attention_attribution(\n",
        "        item['documents'],\n",
        "        item['question'],\n",
        "        rag.model,\n",
        "        rag.tokenizer,\n",
        "        rag.device\n",
        "    )\n",
        "    print(\"Attention-based scores:\")\n",
        "    for i, doc_id in enumerate(item['document_ids']):\n",
        "        print(f\"  Document {doc_id}: {attention_attr[i]:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Attention attribution failed: {e}\")\n",
        "    attention_attr = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualize Attribution Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all attribution results\n",
        "attribution_results = {\n",
        "    'Leave-One-Out': loo_attributions,\n",
        "    'Permutation Shapley': perm_shapley,\n",
        "}\n",
        "\n",
        "if mc_shapley is not None:\n",
        "    attribution_results['Monte Carlo Shapley'] = mc_shapley\n",
        "if kernel_shap is not None:\n",
        "    attribution_results['Kernel SHAP'] = kernel_shap\n",
        "if exact_shap is not None:\n",
        "    attribution_results['Exact Shapley'] = exact_shap\n",
        "if attention_attr is not None:\n",
        "    attribution_results['Attention-based'] = attention_attr\n",
        "\n",
        "# Prepare data for visualization\n",
        "doc_ids = item['document_ids']\n",
        "n_methods = len(attribution_results)\n",
        "\n",
        "# Create comparison plot\n",
        "fig, axes = plt.subplots(1, n_methods, figsize=(6*n_methods, 6))\n",
        "if n_methods == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, (method_name, attributions) in enumerate(attribution_results.items()):\n",
        "    ax = axes[idx]\n",
        "    scores = [attributions[i] for i in range(len(doc_ids))]\n",
        "    \n",
        "    # Normalize scores for better visualization\n",
        "    scores_norm = np.array(scores)\n",
        "    if scores_norm.max() != scores_norm.min():\n",
        "        scores_norm = (scores_norm - scores_norm.min()) / (scores_norm.max() - scores_norm.min())\n",
        "    \n",
        "    bars = ax.bar(range(len(doc_ids)), scores_norm, color='steelblue', alpha=0.7)\n",
        "    ax.set_xlabel('Document', fontsize=12)\n",
        "    ax.set_ylabel('Normalized Attribution Score', fontsize=12)\n",
        "    ax.set_title(method_name, fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(range(len(doc_ids)))\n",
        "    ax.set_xticklabels(doc_ids)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, (bar, score) in enumerate(zip(bars, scores)):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{score:.3f}',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.suptitle(f'Document Attribution Comparison\\nQuestion: {item[\"question\"][:60]}...', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create heatmap comparing all methods\n",
        "attribution_matrix = []\n",
        "method_names = []\n",
        "for method_name, attributions in attribution_results.items():\n",
        "    scores = [attributions[i] for i in range(len(doc_ids))]\n",
        "    # Normalize\n",
        "    scores_norm = np.array(scores)\n",
        "    if scores_norm.max() != scores_norm.min():\n",
        "        scores_norm = (scores_norm - scores_norm.min()) / (scores_norm.max() - scores_norm.min())\n",
        "    attribution_matrix.append(scores_norm)\n",
        "    method_names.append(method_name)\n",
        "\n",
        "attribution_matrix = np.array(attribution_matrix)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(\n",
        "    attribution_matrix,\n",
        "    xticklabels=doc_ids,\n",
        "    yticklabels=method_names,\n",
        "    cmap='RdYlGn',\n",
        "    cbar_kws={'label': 'Normalized Attribution Score'},\n",
        "    annot=True,\n",
        "    fmt='.3f',\n",
        "    linewidths=0.5\n",
        ")\n",
        "plt.xlabel('Documents', fontsize=12)\n",
        "plt.ylabel('Attribution Method', fontsize=12)\n",
        "plt.title('Attribution Methods Comparison Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rank documents by each method\n",
        "print(\"Document Rankings by Attribution Method:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for method_name, attributions in attribution_results.items():\n",
        "    # Get sorted indices\n",
        "    sorted_indices = sorted(range(len(doc_ids)), key=lambda i: attributions[i], reverse=True)\n",
        "    \n",
        "    print(f\"\\n{method_name}:\")\n",
        "    for rank, idx in enumerate(sorted_indices, 1):\n",
        "        doc_id = doc_ids[idx]\n",
        "        score = attributions[idx]\n",
        "        doc_preview = item['documents'][idx][:50] + \"...\"\n",
        "        print(f\"  {rank}. Document {doc_id} (score: {score:.4f}): {doc_preview}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"=\" * 80)\n",
        "print(\"ATTRIBUTION ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nQuery: {item['question']}\")\n",
        "print(f\"Target Response: {target_response}\")\n",
        "print(f\"Expected Answer: {item['answer']}\")\n",
        "print(f\"\\nNumber of documents: {len(item['documents'])}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"Top 3 Most Important Documents by Each Method:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for method_name, attributions in attribution_results.items():\n",
        "    sorted_indices = sorted(range(len(doc_ids)), key=lambda i: attributions[i], reverse=True)\n",
        "    top_3 = sorted_indices[:3]\n",
        "    \n",
        "    print(f\"\\n{method_name}:\")\n",
        "    for rank, idx in enumerate(top_3, 1):\n",
        "        doc_id = doc_ids[idx]\n",
        "        score = attributions[idx]\n",
        "        print(f\"  {rank}. Document {doc_id}: {score:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
