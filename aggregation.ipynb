{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0c8bee",
   "metadata": {},
   "source": [
    "# **Testing Aggregation Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae22a84",
   "metadata": {},
   "source": [
    "We know how to find attributions `1-to-1`. We'd like to find attributions:\n",
    "- `many-to-1` (from an entire specific context document to 1 important token) or \n",
    "- `many-to-many` (from an entire specific context document to multiple tokens or the entire answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a10a74",
   "metadata": {},
   "source": [
    "### **Imports and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8584c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31d51c",
   "metadata": {},
   "source": [
    "### **Load Model and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ef5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Llama-3.2-1B model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\"  # Use eager attention to enable output_attentions\n",
    ")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters() / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf1660",
   "metadata": {},
   "source": [
    "### **Run Generation Task**\n",
    "\n",
    "- We'll ask the question: `What is the Capital City of Latvia?` (9 input tokens)\n",
    "- And will receive the answer: `The capital city of Latvia is Riga.` (9 output tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"What is the Capital City of Latvia?\"\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual autoregressive generation for 9 tokens using embeddings\n",
    "# This approach allows for gradient-based explainability methods later\n",
    "num_tokens_to_generate = 9\n",
    "\n",
    "# Get embeddings layer and its weight matrix\n",
    "embeddings_layer = model.get_input_embeddings()\n",
    "embedding_matrix = embeddings_layer.weight  # Shape: (vocab_size, embedding_dim)\n",
    "\n",
    "# Manually create embeddings by indexing the weight matrix\n",
    "# This ensures we build a proper computation graph for gradient methods\n",
    "current_token_embeddings = embedding_matrix[input_ids]  # Shape: (batch, seq_len, embedding_dim)\n",
    "\n",
    "generated_tokens = []\n",
    "\n",
    "print(\"Starting manual autoregressive generation using embeddings...\")\n",
    "print(f\"Initial prompt: {prompt}\")\n",
    "print(f\"Initial tokens: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
    "print(f\"Embedding shape: {current_token_embeddings.shape}\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(num_tokens_to_generate):\n",
    "        # Forward pass with custom embeddings\n",
    "        outputs = model(inputs_embeds=current_token_embeddings)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the logits for the last position\n",
    "        target_logits = logits[0, -1]\n",
    "        \n",
    "        # Get the predicted token (greedy decoding)\n",
    "        predicted_token_id = target_logits.argmax(dim=-1)\n",
    "        \n",
    "        # Store the generated token\n",
    "        generated_tokens.append(predicted_token_id.item())\n",
    "        \n",
    "        # Decode and print\n",
    "        predicted_token = tokenizer.decode([predicted_token_id])\n",
    "        print(f\"Step {step + 1}: Generated token '{predicted_token}' (ID: {predicted_token_id.item()})\")\n",
    "        \n",
    "        # Get embedding for the new token and append to the sequence\n",
    "        new_token_embedding = embedding_matrix[predicted_token_id].unsqueeze(0).unsqueeze(0)\n",
    "        current_token_embeddings = torch.cat([current_token_embeddings, new_token_embedding], dim=1)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Generation complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nGenerated token IDs: {generated_tokens}\")\n",
    "print(f\"Generated tokens: {tokenizer.convert_ids_to_tokens(generated_tokens)}\")\n",
    "print(f\"\\nGenerated text: {tokenizer.decode(generated_tokens)}\")\n",
    "print(f\"\\nFinal embedding shape: {current_token_embeddings.shape}\")\n",
    "print(f\"\\nFull text (prompt + generation):\")\n",
    "# Reconstruct full token IDs for decoding\n",
    "full_token_ids = torch.cat([input_ids[0], torch.tensor(generated_tokens, device=device)])\n",
    "print(f\"{tokenizer.decode(full_token_ids)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
