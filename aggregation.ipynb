{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0c8bee",
   "metadata": {},
   "source": [
    "# **Testing Aggregation Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae22a84",
   "metadata": {},
   "source": [
    "We know how to find attributions `1-to-1`. We'd like to find attributions:\n",
    "- `many-to-1` (from an entire specific context document to 1 important token) or \n",
    "- `many-to-many` (from an entire specific context document to multiple tokens or the entire answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a10a74",
   "metadata": {},
   "source": [
    "### **Imports and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8584c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31d51c",
   "metadata": {},
   "source": [
    "### **Load Model and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ef5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Llama-3.2-1B model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\"  # Use eager attention to enable output_attentions\n",
    ")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters() / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf1660",
   "metadata": {},
   "source": [
    "### **Run Generation Task**\n",
    "\n",
    "- We'll ask the question: `What is the Capital City of Latvia?` (9 input tokens)\n",
    "- And will receive the answer: `The capital city of Latvia is Riga.` (9 output tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"What is the Capital City of Latvia?\"\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual autoregressive generation for 9 tokens using embeddings\n",
    "# This approach allows for gradient-based explainability methods later\n",
    "num_tokens_to_generate = 9\n",
    "\n",
    "# Get embeddings layer and its weight matrix\n",
    "embeddings_layer = model.get_input_embeddings()\n",
    "embedding_matrix = embeddings_layer.weight  # Shape: (vocab_size, embedding_dim)\n",
    "\n",
    "# Manually create embeddings by indexing the weight matrix\n",
    "# This ensures we build a proper computation graph for gradient methods\n",
    "current_token_embeddings = embedding_matrix[input_ids]  # Shape: (batch, seq_len, embedding_dim)\n",
    "\n",
    "generated_tokens = []\n",
    "\n",
    "print(\"Starting manual autoregressive generation using embeddings...\")\n",
    "print(f\"Initial prompt: {prompt}\")\n",
    "print(f\"Initial tokens: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
    "print(f\"Embedding shape: {current_token_embeddings.shape}\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(num_tokens_to_generate):\n",
    "        # Forward pass with custom embeddings\n",
    "        outputs = model(inputs_embeds=current_token_embeddings)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the logits for the last position\n",
    "        target_logits = logits[0, -1]\n",
    "        \n",
    "        # Get the predicted token (greedy decoding)\n",
    "        predicted_token_id = target_logits.argmax(dim=-1)\n",
    "        \n",
    "        # Store the generated token\n",
    "        generated_tokens.append(predicted_token_id.item())\n",
    "        \n",
    "        # Decode and print\n",
    "        predicted_token = tokenizer.decode([predicted_token_id])\n",
    "        print(f\"Step {step + 1}: Generated token '{predicted_token}' (ID: {predicted_token_id.item()})\")\n",
    "        \n",
    "        # Get embedding for the new token and append to the sequence\n",
    "        new_token_embedding = embedding_matrix[predicted_token_id].unsqueeze(0).unsqueeze(0)\n",
    "        current_token_embeddings = torch.cat([current_token_embeddings, new_token_embedding], dim=1)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Generation complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nGenerated token IDs: {generated_tokens}\")\n",
    "print(f\"Generated tokens: {tokenizer.convert_ids_to_tokens(generated_tokens)}\")\n",
    "print(f\"\\nGenerated text: {tokenizer.decode(generated_tokens)}\")\n",
    "print(f\"\\nFinal embedding shape: {current_token_embeddings.shape}\")\n",
    "print(f\"\\nFull text (prompt + generation):\")\n",
    "# Reconstruct full token IDs for decoding\n",
    "full_token_ids = torch.cat([input_ids[0], torch.tensor(generated_tokens, device=device)])\n",
    "print(f\"{tokenizer.decode(full_token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23268ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n\\n{'#'*80}\")\n",
    "print(\"### GRADIENT-BASED GENERATION WITH ATTRIBUTION TRACKING ###\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "def generate_one_token_with_attribution(model, original_input_ids, generated_token_ids):\n",
    "    \"\"\"\n",
    "    Generate one token and compute attribution scores for the original input tokens.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        original_input_ids: Original input token IDs (tensor)\n",
    "        generated_token_ids: List of already generated token IDs\n",
    "    \n",
    "    Returns:\n",
    "        attribution_scores: Attribution scores for each original input token\n",
    "        predicted_token_id: The next predicted token ID\n",
    "    \"\"\"\n",
    "    # Combine original input and generated tokens\n",
    "    if len(generated_token_ids) > 0:\n",
    "        generated_tensor = torch.tensor(generated_token_ids, device=device).unsqueeze(0)\n",
    "        combined_input_ids = torch.cat([original_input_ids, generated_tensor], dim=1)\n",
    "    else:\n",
    "        combined_input_ids = original_input_ids\n",
    "    \n",
    "    # Get embeddings layer and weight matrix\n",
    "    embeddings_layer = model.get_input_embeddings()\n",
    "    embedding_matrix = embeddings_layer.weight\n",
    "    \n",
    "    # Enable gradient computation\n",
    "    with torch.enable_grad():\n",
    "        # Create embeddings by indexing the weight matrix\n",
    "        token_embeddings = embedding_matrix[combined_input_ids]\n",
    "        \n",
    "        # Detach and clone to break any existing graph, then enable gradients\n",
    "        token_embeddings = token_embeddings.detach().clone()\n",
    "        token_embeddings.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass with custom embeddings\n",
    "        outputs = model(inputs_embeds=token_embeddings)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the logits for the last position\n",
    "        target_logits = logits[0, -1]\n",
    "        \n",
    "        # Get the predicted token\n",
    "        predicted_token_id = target_logits.argmax(dim=-1)\n",
    "        \n",
    "        # Compute gradients of the predicted token's logit w.r.t. input embeddings\n",
    "        target_score = target_logits[predicted_token_id]\n",
    "        target_score.backward()\n",
    "        \n",
    "        # Get gradients\n",
    "        gradients = token_embeddings.grad\n",
    "        \n",
    "        # Check if gradients were computed\n",
    "        if gradients is None:\n",
    "            raise RuntimeError(\"Gradients were not computed.\")\n",
    "        \n",
    "        # Compute attribution scores (L2 norm of gradients) for ORIGINAL input tokens only\n",
    "        original_input_length = original_input_ids.shape[1]\n",
    "        attribution_scores = gradients[0, :original_input_length].norm(dim=-1).cpu().detach().numpy()\n",
    "    \n",
    "    return attribution_scores, predicted_token_id.item()\n",
    "\n",
    "# Run iterative generation with attribution tracking\n",
    "print(\"Starting gradient-based autoregressive generation with attribution tracking...\")\n",
    "print(f\"Generating {num_tokens_to_generate} tokens...\\n\")\n",
    "\n",
    "# Store results as list of tuples: (generated_token_id, attribution_scores)\n",
    "generation_results = []\n",
    "current_generated_tokens = []\n",
    "\n",
    "for step in range(num_tokens_to_generate):\n",
    "    # Generate one token with attribution\n",
    "    attr_scores, next_token_id = generate_one_token_with_attribution(\n",
    "        model, \n",
    "        input_ids, \n",
    "        current_generated_tokens\n",
    "    )\n",
    "    \n",
    "    # Store the result\n",
    "    generation_results.append((next_token_id, attr_scores))\n",
    "    current_generated_tokens.append(next_token_id)\n",
    "    \n",
    "    # Decode and print\n",
    "    predicted_token = tokenizer.decode([next_token_id])\n",
    "    print(f\"Step {step + 1}: Generated '{predicted_token}' (ID: {next_token_id})\")\n",
    "    print(f\"  Attribution scores shape: {attr_scores.shape}\")\n",
    "    print(f\"  Top 3 attributed input tokens:\")\n",
    "    \n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    top_3_indices = np.argsort(attr_scores)[-3:][::-1]\n",
    "    for idx in top_3_indices:\n",
    "        print(f\"    - '{input_tokens[idx]}' (pos {idx}): {attr_scores[idx]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Final summary\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"GRADIENT-BASED GENERATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGenerated {len(generation_results)} tokens with attribution scores\")\n",
    "print(f\"\\nGenerated token IDs: {[token_id for token_id, _ in generation_results]}\")\n",
    "print(f\"Generated tokens: {tokenizer.convert_ids_to_tokens([token_id for token_id, _ in generation_results])}\")\n",
    "print(f\"\\nGenerated text: {tokenizer.decode([token_id for token_id, _ in generation_results])}\")\n",
    "print(f\"\\nAttribution scores stored for each generation step.\")\n",
    "print(f\"Access as: generation_results[step] = (token_id, attribution_scores)\")\n",
    "\n",
    "# ====== Process attribution scores per input token across all generated output tokens ======\n",
    "# \"processed_results\" will be a list of tuples: (input_token, input_token_id, [attr_score_for_each_output_token])\n",
    "\n",
    "input_token_ids = input_ids[0].tolist()\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_token_ids)\n",
    "\n",
    "# Number of output tokens generated\n",
    "num_output_tokens = len(generation_results)\n",
    "\n",
    "# Build a matrix attribution_scores_matrix[output_j][input_t]\n",
    "attribution_scores_matrix = np.stack([attr_scores for (_, attr_scores) in generation_results], axis=0)  # shape: (num_output_tokens, num_input_tokens)\n",
    "\n",
    "# Now for each input token, collect its scores across all output tokens\n",
    "processed_results = []\n",
    "for t, (tok, tok_id) in enumerate(zip(input_tokens, input_token_ids)):\n",
    "    # scores for this input token across all output tokens (j=0..num_output_tokens-1)\n",
    "    per_output_scores = attribution_scores_matrix[:, t].tolist()\n",
    "    processed_results.append( (tok, tok_id, per_output_scores) )\n",
    "\n",
    "# processed_results[n] = (input_token, input_token_id, [attribution_to_output_j for j in output_tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad216c",
   "metadata": {},
   "source": [
    "### **Do Aggregation**\n",
    "Look at 3 slices of inputs:\n",
    "- Token 1-3 (What is the)\n",
    "- Token 4-5 (Capital City)\n",
    "- Token 6-7 (of Latvia)'\n",
    "\n",
    "and compare their aggregated attribution scores for the output tokens 6-7 (R iga)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a71cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_slices = {\n",
    "    \"What is the (tokens 1-3)\": [1:4],\n",
    "    \"Capital City (tokens 4-5)\": [4:6],\n",
    "    \"of Latvia (tokens 6-7)\": [6:7]\n",
    "}\n",
    "\n",
    "# Output tokens to analyze: tokens 6-7 -> indices 5, 6 (0-indexed in generation_results)\n",
    "output_token_indices = [6, 7]\n",
    "\n",
    "# Get the actual output tokens for labeling\n",
    "output_tokens_for_plot = [tokenizer.decode([generation_results[i][0]]) for i in output_token_indices]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AGGREGATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nInput slices defined:\")\n",
    "for name, indices in input_slices.items():\n",
    "    tokens = [processed_results[i][0] for i in indices]\n",
    "    print(f\"  {name}: {tokens}\")\n",
    "\n",
    "print(f\"\\nAnalyzing attribution to output tokens {output_token_indices[0]+1} and {output_token_indices[1]+1}:\")\n",
    "for i, idx in enumerate(output_token_indices):\n",
    "    print(f\"  Output token {idx+1}: '{output_tokens_for_plot[i]}' (ID: {generation_results[idx][0]})\")\n",
    "\n",
    "# Compute aggregated (averaged) attribution scores for each input slice and output token\n",
    "aggregated_scores = {}\n",
    "\n",
    "for slice_name, input_indices in input_slices.items():\n",
    "    slice_scores = []\n",
    "    \n",
    "    for output_idx in output_token_indices:\n",
    "        # Get attribution scores for this output token from all input tokens in the slice\n",
    "        scores_for_output = []\n",
    "        for input_idx in input_indices:\n",
    "            # processed_results[input_idx] = (token, token_id, [scores_for_each_output])\n",
    "            score = processed_results[input_idx][2][output_idx]\n",
    "            scores_for_output.append(score)\n",
    "        \n",
    "        # Average the scores across the input tokens in this slice\n",
    "        avg_score = np.mean(scores_for_output)\n",
    "        slice_scores.append(avg_score)\n",
    "    \n",
    "    aggregated_scores[slice_name] = slice_scores\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGGREGATED ATTRIBUTION SCORES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for slice_name, scores in aggregated_scores.items():\n",
    "    print(f\"{slice_name}:\")\n",
    "    for i, (output_idx, score) in enumerate(zip(output_token_indices, scores)):\n",
    "        print(f\"  → Output token {output_idx+1} ('{output_tokens_for_plot[i]}'): {score:.6f}\")\n",
    "    print()\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "slice_names = list(aggregated_scores.keys())\n",
    "num_slices = len(slice_names)\n",
    "num_outputs = len(output_token_indices)\n",
    "\n",
    "# Set up bar positions\n",
    "x = np.arange(num_outputs)\n",
    "width = 0.25  # Width of each bar\n",
    "\n",
    "# Plot bars for each slice\n",
    "colors = ['#4472C4', '#ED7D31', '#A5A5A5']\n",
    "for i, slice_name in enumerate(slice_names):\n",
    "    scores = aggregated_scores[slice_name]\n",
    "    offset = (i - num_slices/2 + 0.5) * width\n",
    "    ax.bar(x + offset, scores, width, label=slice_name, color=colors[i], alpha=0.8)\n",
    "\n",
    "# Customize plot\n",
    "ax.set_xlabel('Output Tokens', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Average Attribution Score', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Aggregated Attribution Scores: Input Slices → Output Tokens 6-7', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"Token {idx+1}\\n'{tok}'\" for idx, tok in zip(output_token_indices, output_tokens_for_plot)])\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a heatmap for better visualization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HEATMAP VISUALIZATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = np.array([aggregated_scores[name] for name in slice_names])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "im = ax.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(num_outputs))\n",
    "ax.set_yticks(np.arange(num_slices))\n",
    "ax.set_xticklabels([f\"Token {idx+1}: '{tok}'\" for idx, tok in zip(output_token_indices, output_tokens_for_plot)])\n",
    "ax.set_yticklabels(slice_names)\n",
    "\n",
    "# Rotate the x-axis labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Average Attribution Score', rotation=270, labelpad=20, fontsize=11)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(num_slices):\n",
    "    for j in range(num_outputs):\n",
    "        text = ax.text(j, i, f'{heatmap_data[i, j]:.4f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_title('Attribution Heatmap: Input Slices → Output Tokens 6-7', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Output Tokens', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Input Token Slices', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Aggregation analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740cbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
