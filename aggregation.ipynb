{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0c8bee",
   "metadata": {},
   "source": [
    "# **Testing Aggregation Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae22a84",
   "metadata": {},
   "source": [
    "We know how to find attributions `1-to-1`. We'd like to find attributions:\n",
    "- `many-to-1` (from an entire specific context document to 1 important token) or \n",
    "- `many-to-many` (from an entire specific context document to multiple tokens or the entire answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a10a74",
   "metadata": {},
   "source": [
    "### **Imports and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8584c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31d51c",
   "metadata": {},
   "source": [
    "### **Load Model and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ef5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Llama-3.2-1B model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\"  # Use eager attention to enable output_attentions\n",
    ")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters() / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf1660",
   "metadata": {},
   "source": [
    "### **Run Generation Task**\n",
    "\n",
    "- We'll ask the question: `What is the Capital City of Latvia?` (9 input tokens)\n",
    "- And will receive the answer: `The capital city of Latvia is Riga.` (9 output tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"What is the Capital City of Latvia?\"\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual autoregressive generation for 9 tokens using embeddings\n",
    "# This approach allows for gradient-based explainability methods later\n",
    "num_tokens_to_generate = 9\n",
    "\n",
    "# Get embeddings layer and its weight matrix\n",
    "embeddings_layer = model.get_input_embeddings()\n",
    "embedding_matrix = embeddings_layer.weight  # Shape: (vocab_size, embedding_dim)\n",
    "\n",
    "# Manually create embeddings by indexing the weight matrix\n",
    "# This ensures we build a proper computation graph for gradient methods\n",
    "current_token_embeddings = embedding_matrix[input_ids]  # Shape: (batch, seq_len, embedding_dim)\n",
    "\n",
    "generated_tokens = []\n",
    "\n",
    "print(\"Starting manual autoregressive generation using embeddings...\")\n",
    "print(f\"Initial prompt: {prompt}\")\n",
    "print(f\"Initial tokens: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
    "print(f\"Embedding shape: {current_token_embeddings.shape}\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(num_tokens_to_generate):\n",
    "        # Forward pass with custom embeddings\n",
    "        outputs = model(inputs_embeds=current_token_embeddings)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the logits for the last position\n",
    "        target_logits = logits[0, -1]\n",
    "        \n",
    "        # Get the predicted token (greedy decoding)\n",
    "        predicted_token_id = target_logits.argmax(dim=-1)\n",
    "        \n",
    "        # Store the generated token\n",
    "        generated_tokens.append(predicted_token_id.item())\n",
    "        \n",
    "        # Decode and print\n",
    "        predicted_token = tokenizer.decode([predicted_token_id])\n",
    "        print(f\"Step {step + 1}: Generated token '{predicted_token}' (ID: {predicted_token_id.item()})\")\n",
    "        \n",
    "        # Get embedding for the new token and append to the sequence\n",
    "        new_token_embedding = embedding_matrix[predicted_token_id].unsqueeze(0).unsqueeze(0)\n",
    "        current_token_embeddings = torch.cat([current_token_embeddings, new_token_embedding], dim=1)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Generation complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nGenerated token IDs: {generated_tokens}\")\n",
    "print(f\"Generated tokens: {tokenizer.convert_ids_to_tokens(generated_tokens)}\")\n",
    "print(f\"\\nGenerated text: {tokenizer.decode(generated_tokens)}\")\n",
    "print(f\"\\nFinal embedding shape: {current_token_embeddings.shape}\")\n",
    "print(f\"\\nFull text (prompt + generation):\")\n",
    "# Reconstruct full token IDs for decoding\n",
    "full_token_ids = torch.cat([input_ids[0], torch.tensor(generated_tokens, device=device)])\n",
    "print(f\"{tokenizer.decode(full_token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23268ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n\\n{'#'*80}\")\n",
    "print(\"### GRADIENT-BASED GENERATION WITH ATTRIBUTION TRACKING ###\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "def generate_one_token_with_attribution(model, original_input_ids, generated_token_ids):\n",
    "    \"\"\"\n",
    "    Generate one token and compute attribution scores for the original input tokens.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        original_input_ids: Original input token IDs (tensor)\n",
    "        generated_token_ids: List of already generated token IDs\n",
    "    \n",
    "    Returns:\n",
    "        attribution_scores: Attribution scores for each original input token\n",
    "        predicted_token_id: The next predicted token ID\n",
    "    \"\"\"\n",
    "    # Combine original input and generated tokens\n",
    "    if len(generated_token_ids) > 0:\n",
    "        generated_tensor = torch.tensor(generated_token_ids, device=device).unsqueeze(0)\n",
    "        combined_input_ids = torch.cat([original_input_ids, generated_tensor], dim=1)\n",
    "    else:\n",
    "        combined_input_ids = original_input_ids\n",
    "    \n",
    "    # Get embeddings layer and weight matrix\n",
    "    embeddings_layer = model.get_input_embeddings()\n",
    "    embedding_matrix = embeddings_layer.weight\n",
    "    \n",
    "    # Enable gradient computation\n",
    "    with torch.enable_grad():\n",
    "        # Create embeddings by indexing the weight matrix\n",
    "        token_embeddings = embedding_matrix[combined_input_ids]\n",
    "        \n",
    "        # Detach and clone to break any existing graph, then enable gradients\n",
    "        token_embeddings = token_embeddings.detach().clone()\n",
    "        token_embeddings.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass with custom embeddings\n",
    "        outputs = model(inputs_embeds=token_embeddings)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the logits for the last position\n",
    "        target_logits = logits[0, -1]\n",
    "        \n",
    "        # Get the predicted token\n",
    "        predicted_token_id = target_logits.argmax(dim=-1)\n",
    "        \n",
    "        # Compute gradients of the predicted token's logit w.r.t. input embeddings\n",
    "        target_score = target_logits[predicted_token_id]\n",
    "        target_score.backward()\n",
    "        \n",
    "        # Get gradients\n",
    "        gradients = token_embeddings.grad\n",
    "        \n",
    "        # Check if gradients were computed\n",
    "        if gradients is None:\n",
    "            raise RuntimeError(\"Gradients were not computed.\")\n",
    "        \n",
    "        # Compute attribution scores (L2 norm of gradients) for ORIGINAL input tokens only\n",
    "        original_input_length = original_input_ids.shape[1]\n",
    "        attribution_scores = gradients[0, :original_input_length].norm(dim=-1).cpu().detach().numpy()\n",
    "    \n",
    "    return attribution_scores, predicted_token_id.item()\n",
    "\n",
    "# Run iterative generation with attribution tracking\n",
    "print(\"Starting gradient-based autoregressive generation with attribution tracking...\")\n",
    "print(f\"Generating {num_tokens_to_generate} tokens...\\n\")\n",
    "\n",
    "# Store results as list of tuples: (generated_token_id, attribution_scores)\n",
    "generation_results = []\n",
    "current_generated_tokens = []\n",
    "\n",
    "for step in range(num_tokens_to_generate):\n",
    "    # Generate one token with attribution\n",
    "    attr_scores, next_token_id = generate_one_token_with_attribution(\n",
    "        model, \n",
    "        input_ids, \n",
    "        current_generated_tokens\n",
    "    )\n",
    "    \n",
    "    # Store the result\n",
    "    generation_results.append((next_token_id, attr_scores))\n",
    "    current_generated_tokens.append(next_token_id)\n",
    "    \n",
    "    # Decode and print\n",
    "    predicted_token = tokenizer.decode([next_token_id])\n",
    "    print(f\"Step {step + 1}: Generated '{predicted_token}' (ID: {next_token_id})\")\n",
    "    print(f\"  Attribution scores shape: {attr_scores.shape}\")\n",
    "    print(f\"  Top 3 attributed input tokens:\")\n",
    "    \n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    top_3_indices = np.argsort(attr_scores)[-3:][::-1]\n",
    "    for idx in top_3_indices:\n",
    "        print(f\"    - '{input_tokens[idx]}' (pos {idx}): {attr_scores[idx]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Final summary\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"GRADIENT-BASED GENERATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGenerated {len(generation_results)} tokens with attribution scores\")\n",
    "print(f\"\\nGenerated token IDs: {[token_id for token_id, _ in generation_results]}\")\n",
    "print(f\"Generated tokens: {tokenizer.convert_ids_to_tokens([token_id for token_id, _ in generation_results])}\")\n",
    "print(f\"\\nGenerated text: {tokenizer.decode([token_id for token_id, _ in generation_results])}\")\n",
    "print(f\"\\nAttribution scores stored for each generation step.\")\n",
    "print(f\"Access as: generation_results[step] = (token_id, attribution_scores)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
